---
title: "Semana 4 - Semana de lectura"
categories:
  - Weekly Log
tags:
  - ROS2
  - Aerostack2
  - Machine Learning
  - Neural Networks
---

En esta semana nos dedicamos a un contexto más teórico, aunque paralelamente, seguimos tratando de "domesticar" el drone con AeroStack2.


## Index
* [TFM Deeplearning-autonomous_driving](#tfm-deeplearning-autonomous_driving)
* [DeepPilot: A CNN for Autonomous Drone Racing](#deeppilot-a-cnn-for-autonomous-drone-racing)
    * [Framework](#framework-propuesto)
* [Aerostack2](#aerostack2)

## TFM Deeplearning-autonomous_driving
Primero, empezamos leyendo el (TFM, DE Enrique Shinohara)[https://gsyc.urjc.es/jmplaza/students/tfm-deeplearning-autonomous_driving-enrique_shinohara-2023.pdf] que presenta un enfoque de aprendizaje profundo en conducción autónoma con redes neuronales de extremo a extremo. Nos será especialmente útil, ya que también utilizaremos la red neuronal PilotNet como primera aproximación a nuestro proyecto.

* Un enfoque interesante está en el proyecto de Felipe Codevilla "End-to-end driving via conditional imitation learning", donde se entrena un vehículo con un enfoque extremo a extremo, donde la percepción y la toma de decisiones la realiza una arquitectura de redes neuronales, con la peculiaridad de que se pueden introducir comandos desde fuera.

* **PilotNet:** Consta de 9 capas, las cuales incluyen una capa de normalización, 5 capas convolucionales y 4 capas de redes neuronales totalmente conectadas o densas.

* Otro aspecto interesante es cómo se incorpora la capa de velocidades previas junto al modelo.

* Reducción y redimensión de píxeles en la imagen para agilizar el entrenamiento, además de tener en cuenta la forma de equilibrar los datos de la manera correcta.

* **Albumentation:** Biblioteca que se utiliza durante el entrenamiento con el fin de aumentar la variedad del conjunto de datos permitiendo cambiar la iluminación, desenfocarla o añadir condiciones temporales.

---
---

## DeepPilot: A CNN for Autonomous Drone Racing

El fundamento de esta CNN (Convolutional Neural Network) es que recibe imágenes de la cámara del drone y predice 4 órdenes de vuelo que representan la posición angular del drone en yaw, pitch y roll, y la componente vertical de la velocidad, refiriéndose a la altitud.

<figure class="align-center" style="width:80%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post4/estructuraDeepPilot.png" alt="">
  <figcaption>Deep Pilot Architecture</figcaption>
</figure>

También se plantea el uso de imágenes consecutivas como entrada, para añadir una tendencia de movimiento y reducir el número de capas. Primero se hace una aproximación con 1 frame, pero haciendo pruebas con 2, 4, 6 y 8 frames se observa que al añadir más frames a la entrada, se actúa con un proceso de memoria, ya que contienen una tendencia de movimiento.

Otro aspecto importante es que se desacoplan yaw y la altitud de pitch y roll. Esta idea está basada en redes que trabajan con VisualSLAM y odometría.

* (Datasets de entrenamiento para modelo DeepPilot)[https://github.com/QuetzalCpp/DeepPilot.git]

* También se presentan varios algoritmos donde los drones completaron las pruebas de la IROS Autonomous Drone Racing 2017. Un aspecto importante a tener en cuenta en el diseño del software de un dron es que simultáneamente hay que considerar la localización, la detección, el control y el cálculo de la ruta o path planning.

* Otros tipos de CNN se dividen en 2 submódulos: una red se encarga de procesar las imágenes como entrada y devuelve una lista de puntos, y la segunda tendrá como entrada estos puntos y devolverá las velocidades.

### Framework propuesto:
1. Adquirir la imagen del drone.
2. Generar una imagen mosaico con cada 5 frames.
3. Predecir los comandos de vuelo con DeepPilot.
4. Implementar un filtro de salida para suavizar los comandos de vuelo.

Arquitectura DeepPilot: Se basa en tener 3 modelos que corren paralelamente, uno para pitch y roll, otro para yaw y otro para la altitud. Debido a la flexibilidad de Keras y TensorFlow, se pueden ejecutar a la vez estos 3 modelos con la misma arquitectura (la única diferencia serán los pesos). Cada rama tendrá 4 capas convolucionales, 3 módulos de intercepción, 1 capa totalmente conectada y 1 capa de regresión. Cuando se entrena esta red neuronal, a cada módulo se le pasa un conjunto de datos donde el movimiento predominante sea el que se quiere entrenar.

<figure class="align-center" style="width:80%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post4/arquitectura_DeepPilot.png" alt="">
  <figcaption>Deep Pilot Architecture</figcaption>
</figure>

* **Filtro de ruido:** El filtro EMA ayuda a reducir el ruido de la predicción a la vez que previene oscilaciones y sacudidas.
* Resultados de comparación de Deepilot vs PoseNet:

<figure class="align-center" style="width:80%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post4/deepPilot_vs_PoseNet.png" alt="">
  <figcaption>Representación de resultados</figcaption>
</figure>

---
---

## Aerostack2

