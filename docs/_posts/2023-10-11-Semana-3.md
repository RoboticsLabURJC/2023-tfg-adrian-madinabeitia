---
title: "Semana 3 - Rosbags + pytorch"
categories:
  - Weekly Log
tags:
  - github pages
  - gazebo
  - ROS2
  - Aerostack2
  - pytorch
---

En esta semana comenzaremos a familiarizarnos a entrenar con un database de rosbags en pytorch. 

## Simulación formula1 sigue lineas
Como aún tenemos problemas con la simulación del drone, empezamos a familiarizarnos con la obtención de conjuntos de datos a partir del mismo ejercicio, pero realizado en un vehículo terrestre. Para hacer el seguimiento de líneas, aprovechamos el código de la asignatura de (robótica de servicio)[https://portanova2002.wixsite.com/robotica-movil/blank-1] del año pasado. Solo tuvimos que adaptar el código de Unibotics a ROS.

## Entrenamiento
### Rosbags
Para obtener el conjunto de datos, utilizamos el siguiente comando:

```bash
ros2 bag record /filtered_img /cmd_vel
```

Solo guardaremos los temas **/filtered_img** y **/cmd_vel**, ya que lo que nos interesa saber es qué velocidades se comandan dependiendo de la imagen; en otras palabras, la entrada sería la imagen y la salida las velocidades del robot.

Al ejecutar el camando tendremos un .yaml con el metadata y un archivo .db3 donde estarán todos los datos guardados.

Para ver qué había dentro del .sdb3, nos descargamos la siguiente (aplicación)[https://sqlitebrowser.org/dl/] e investigamos un poco el SQL.

### pyTorch
#### Dataset
El primer paso a la hora de entrenar una red neuronal es generar el conjunto de datos. Ya lo conseguimos con las rosbags; ahora nos queda dejarlo en un formato que pueda leer PyTorch. Esta respuesta la encontramos en (Stack Overflow)[https://stackoverflow.com/questions/73420147/how-to-read-custom-message-type-using-ros2bag], donde se intentan leer mensajes personalizados, pero con la parte de mensajes estándar nos basta. A partir de este código, accedemos a los parámetros que nos interesan: la matriz de la imagen y la velocidad lineal en el eje x junto con la velocidad angular en el eje z.

En el tratamiento de imágenes para comprobar si las imágenes eran correctas, se pasó del formato de la imagen cruda a uno de tres canales. Primero se visualizó toda la información que se guarda en la rosbag; al ser la codificación BGR8, se redimensionó la imagen con 3 canales, y el resto de parámetros, como el ancho, la altura o el tipo de dato, los obtenemos directamente de esta estructura. Estas operaciones nos permiten ver la imagen si así lo deseamos.

Se buscó información sobre qué era más óptimo para entrenar una red neuronal: si los datos de la imagen en bruto o en RGB. La imagen en bruto tiene la ventaja de capturar más información que la RGB, pero añade complejidad en el manejo de datos. Además, las imágenes RGB están más extendidas en tareas de visión artificial, por lo que, aunque puedan perder datos, suele ser lo más óptimo.


#### pyTorch
Después, seguimos leyendo varios tutoriales de PyTorch para familiarizarnos con el mismo. Como por el momento, no sabía qué red neuronal o técnica de machine learning aplicar, me centré en comprender cómo funciona y en un estudio un poco más teórico.

## Plataforma tello
También seguimos con los problemas de la semana pasada; lo primero a corregir es el problema de las transformaciones (tfs). Me dí cuenta de que era un fallo al llamar a las funciones de movimiento ya que las llamaba fuera del nodo de ROS. Sin embargo al intentar mover el drone, en la odometría tenía el mismo error por lo que subí un (issue)[https://github.com/aerostack2/aerostack2/issues/347] en el repositorio de aerostack2 en busca de ayuda.

## Links utilizados:
* https://file.org/extension/db3
* https://pytorch.org/tutorials/beginner/introyt/trainingyt.html
